{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "sys.path.append('/home/kai/DAST/network')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from DAST_utils import *\n",
    "from DAST_Network import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DASTModel():\n",
    "    def __init__(self, train_datasets: List[str], test_dataset: List[str], data_path: str, is_norm: bool, hyper_parameters: dict, model_save_path: str, model_save_name: str) -> None:\n",
    "        self.TRAIN_DATASETS = train_datasets\n",
    "        self.TEST_DATASETS = test_dataset\n",
    "        self.DATA_PATH = data_path\n",
    "        self.MODEL_SAVE_PATH = model_save_path\n",
    "        self.MODEL_SAVE_NAME = model_save_name\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.Y_train = []\n",
    "        self.Y_test = []\n",
    "        self.HP = hyper_parameters\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.best_predict = []\n",
    "        self.last_predict_y = None\n",
    "        self.train_loss_list = []\n",
    "        self.test_loss_list = []\n",
    "        self.best_mse_loss = 10000.0\n",
    "        self.best_rmse_loss = None\n",
    "        self.best_train_loss = 10000.0\n",
    "        self.norm = '_norm' if is_norm else ''\n",
    "        self.best_model_params = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def RMSE(target, pred):\n",
    "        square_error = (target - pred) ** 2\n",
    "        mse =  (torch.sum(square_error)) / len(target)\n",
    "        rmse = mse ** 0.5\n",
    "        return rmse\n",
    "    \n",
    "    @staticmethod\n",
    "    def MAE(target, pred):\n",
    "        absolute_error = np.abs(target - pred)\n",
    "        return torch.sum(absolute_error) / len(target)\n",
    "    \n",
    "    def _load_x_y(self, folder: str):\n",
    "        y_tmp = np.load(f'{self.DATA_PATH}/{folder}/{folder}_Y.npy')\n",
    "        feature1 = np.load(f'{self.DATA_PATH}/{folder}/{folder}_X{self.norm}_2560.npy')\n",
    "        feature2 = np.load(f'{self.DATA_PATH}/{folder}/{folder}_X{self.norm}_1280.npy')\n",
    "        feature3 = np.load(f'{self.DATA_PATH}/{folder}/{folder}_X{self.norm}_640.npy')\n",
    "        X_train = np.concatenate((feature1, feature2, feature3), axis=2)\n",
    "        return X_train, np.reshape(y_tmp, ((len(y_tmp), -1)))\n",
    "    \n",
    "    def _concate(self):\n",
    "        self.X_train = np.concatenate(self.X_train, axis=0)\n",
    "        self.Y_train = np.concatenate(self.Y_train, axis=0)\n",
    "        self.X_test = np.concatenate(self.X_test, axis=0)\n",
    "        self.Y_test = np.concatenate(self.Y_test, axis=0)\n",
    "\n",
    "    def _load_np(self,):\n",
    "        # train\n",
    "        for folder in self.TRAIN_DATASETS:\n",
    "            X_train, Y_train = self._load_x_y(folder)\n",
    "            self.X_train.append(X_train)\n",
    "            self.Y_train.append(Y_train)\n",
    "        # test\n",
    "        for folder in self.TEST_DATASETS:\n",
    "            X_test, Y_test = self._load_x_y(folder)\n",
    "            self.X_test.append(X_test)\n",
    "            self.Y_test.append(Y_test)\n",
    "        \n",
    "    def _loop_feature(self, X, selected_indices):\n",
    "        extracted_values_list = []\n",
    "        for i in range(7):\n",
    "            for num in selected_indices:\n",
    "                extracted_values = X[:, :, num + 16 * i]\n",
    "                extracted_values_list.append(extracted_values)\n",
    "        result_array = np.stack(extracted_values_list, axis=-1)\n",
    "        return result_array\n",
    "    \n",
    "    def _select_feature(self, selected_indices):\n",
    "        for i in range(len(self.X_train)):\n",
    "            self.X_train[i] = self._loop_feature(self.X_train[i], selected_indices)\n",
    "        for i in range(len(self.X_test)):\n",
    "            self.X_test[i] = self._loop_feature(self.X_test[i], selected_indices)\n",
    "        \n",
    "    def _tensorizing(self):\n",
    "        self.X_train = Variable(torch.Tensor(self.X_train).float())\n",
    "        self.Y_train = Variable(torch.Tensor(self.Y_train).float())\n",
    "        self.X_test = Variable(torch.Tensor(self.X_test).float())\n",
    "        self.Y_test = Variable(torch.Tensor(self.Y_test).float())\n",
    "        \n",
    "    def _get_dataloader(self):\n",
    "        train_dataset = TensorDataset(self.X_train, self.Y_train)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=self.HP['batch_size'], shuffle=False)\n",
    "        test_dataset = TensorDataset(self.X_test, self.Y_test)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=self.HP['batch_size'], shuffle=False)\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def _get_model(self):\n",
    "        model = DAST(self.HP['dim_val_s'], self.HP['dim_attn_s'], self.HP['dim_val_t'], self.HP['dim_attn_t'], self.HP['dim_val'], self.HP['dim_attn'], self.HP['time_step'], self.HP['feature_len'], self.HP['dec_seq_len'], self.HP['output_sequence_length'], self.HP['n_decoder_layers'], self.HP['n_encoder_layers'], self.HP['n_heads'], self.HP['debug'])\n",
    "        model = model.to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.HP['lr'])\n",
    "        criterion = nn.MSELoss()\n",
    "        return model, optimizer, criterion\n",
    "        \n",
    "    def inference(self, selected_indices, pretrain_model_path, pretrain_model_name, output_path):\n",
    "        self._load_np()\n",
    "        self._select_feature(selected_indices)\n",
    "        self._concate()\n",
    "        self._tensorizing()\n",
    "        criterion = nn.MSELoss()\n",
    "        model = DAST(self.HP['dim_val_s'], self.HP['dim_attn_s'], self.HP['dim_val_t'], self.HP['dim_attn_t'], self.HP['dim_val'], self.HP['dim_attn'], self.HP['time_step'], self.HP['feature_len'], self.HP['dec_seq_len'], self.HP['output_sequence_length'], self.HP['n_decoder_layers'], self.HP['n_encoder_layers'], self.HP['n_heads'], self.HP['debug'])\n",
    "        model.load_state_dict(torch.load(f'{pretrain_model_path}/{pretrain_model_name}.pt'))\n",
    "        model.to(self.device)\n",
    "        _, test_loader = self._get_dataloader()\n",
    "        model.eval()\n",
    "        prediction_list = []\n",
    "        with torch.no_grad():\n",
    "            for _ ,(batch_x, _) in enumerate(test_loader):\n",
    "                batch_X = batch_x.to(self.device)\n",
    "                prediction = model(batch_X)\n",
    "                prediction_list.append(prediction)\n",
    "        out_batch_pre = torch.cat(prediction_list).detach().cpu()\n",
    "        rmse_loss = self.RMSE(self.Y_test, out_batch_pre, )\n",
    "        mae_loss = self.MAE(self.Y_test, out_batch_pre, )\n",
    "        test_loss = criterion(out_batch_pre, self.Y_test)\n",
    "        self.test_loss_list.append(test_loss)\n",
    "        # print('rmse_loss = ', f'{rmse_loss.item():.7f}',\n",
    "        #         'mae_loss = ', f'{mae_loss.item():.7f}',\n",
    "        #         'mse_loss = ', f'{test_loss.item():.7f}')\n",
    "        # np.save(f'{output_path}/{pretrain_model_name}_{self.TEST_DATASETS[0]}.npy', out_batch_pre)\n",
    "        return rmse_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../../data/10FEMTO/processed_data/'\n",
    "MODEL_SAVE_NAME = 'Bearing3_pretrain_7'\n",
    "IS_NORM = False\n",
    "MODEL_SAVE_PATH = '../../../model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices = [1, 3, 5, 7, 9, 10, 14]\n",
    "# selected_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "FEATURE_LEN = len(selected_indices)\n",
    "FEATURE_SIZE = 20\n",
    "EMBEDD = 10\n",
    "HYPER_PARAMETERS = {\n",
    "    'batch_size': 256,\n",
    "    'dim_val': FEATURE_SIZE,\n",
    "    'dim_attn': EMBEDD,\n",
    "    'dim_val_t': FEATURE_SIZE,\n",
    "    'dim_attn_t': EMBEDD,\n",
    "    'dim_val_s': FEATURE_SIZE,\n",
    "    'dim_attn_s': EMBEDD,\n",
    "    'n_heads': 4,\n",
    "    'n_decoder_layers': 1,\n",
    "    'n_encoder_layers': 2,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 100,\n",
    "    'time_step': 40,\n",
    "    # limit how many last input used, important!\n",
    "    'dec_seq_len': 6,\n",
    "    'output_sequence_length': 1,\n",
    "    'feature_len': FEATURE_LEN,\n",
    "    'debug': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_MODEL_PATH = '../../../model'\n",
    "NP_OUTPUT = '../../../data/np/'\n",
    "\n",
    "def test_loop():\n",
    "    TRAIN_DATASETS = ['Bearing2_7']\n",
    "    # TEST_DATASET = ['Bearing2_4']\n",
    "    TEST_DATASET = ['Bearing1_3', 'Bearing1_4', 'Bearing1_5', 'Bearing1_6', 'Bearing1_7' ,\\\n",
    "    'Bearing2_3', 'Bearing2_4', 'Bearing2_5', 'Bearing2_6', 'Bearing2_7' ,\\\n",
    "    'Bearing3_3']\n",
    "    DATASETS = ['Bearing1', 'Bearing2', 'Bearing3']\n",
    "    \n",
    "    for dataset in TEST_DATASET:\n",
    "        test_num = dataset[7]\n",
    "        FINETUNE_DATASET = dataset[:8]\n",
    "        for DATASET in DATASETS:\n",
    "            if DATASET[7] != test_num:\n",
    "                PRETRAIN_DATASET = DATASET\n",
    "                PRETRAIN_MODEL_NAME = f'{PRETRAIN_DATASET}_pretrain_{FINETUNE_DATASET}_finetune_7'\n",
    "            else:\n",
    "                PRETRAIN_MODEL_NAME = f'{FINETUNE_DATASET}_pretrain_7'\n",
    "            dast_model = DASTModel(train_datasets=TRAIN_DATASETS, test_dataset=[dataset], data_path=DATA_PATH, is_norm=IS_NORM, hyper_parameters=HYPER_PARAMETERS, model_save_path=MODEL_SAVE_PATH, model_save_name=MODEL_SAVE_NAME)\n",
    "            rmse, mse = dast_model.inference(selected_indices, PRETRAIN_MODEL_PATH, PRETRAIN_MODEL_NAME, NP_OUTPUT)\n",
    "            if (DATASET == FINETUNE_DATASET):\n",
    "                print(dataset, DATASET, 'None', f'{rmse.item():.7f}', f'{mse.item():.7f}')\n",
    "            else:\n",
    "                print(dataset, DATASET, FINETUNE_DATASET, f'{rmse.item():.7f}', f'{mse.item():.7f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearing1_3 Bearing1 None 0.0632105 0.0039956\n",
      "Bearing1_3 Bearing2 Bearing1 0.1519653 0.0230935\n",
      "Bearing1_3 Bearing3 Bearing1 0.0953454 0.0090908\n",
      "Bearing1_4 Bearing1 None 0.2971727 0.0883116\n",
      "Bearing1_4 Bearing2 Bearing1 0.2504566 0.0627285\n",
      "Bearing1_4 Bearing3 Bearing1 0.1300722 0.0169188\n",
      "Bearing1_5 Bearing1 None 0.2064061 0.0426035\n",
      "Bearing1_5 Bearing2 Bearing1 0.2103634 0.0442527\n",
      "Bearing1_5 Bearing3 Bearing1 0.2041798 0.0416894\n",
      "Bearing1_6 Bearing1 None 0.2077157 0.0431458\n",
      "Bearing1_6 Bearing2 Bearing1 0.2147992 0.0461387\n",
      "Bearing1_6 Bearing3 Bearing1 0.2070792 0.0428818\n",
      "Bearing1_7 Bearing1 None 0.1842623 0.0339526\n",
      "Bearing1_7 Bearing2 Bearing1 0.1997703 0.0399082\n",
      "Bearing1_7 Bearing3 Bearing1 0.2176494 0.0473713\n",
      "Bearing2_3 Bearing1 Bearing2 0.2234273 0.0499197\n",
      "Bearing2_3 Bearing2 None 0.2290070 0.0524442\n",
      "Bearing2_3 Bearing3 Bearing2 0.2396014 0.0574088\n",
      "Bearing2_4 Bearing1 Bearing2 0.2063725 0.0425896\n",
      "Bearing2_4 Bearing2 None 0.2127511 0.0452630\n",
      "Bearing2_4 Bearing3 Bearing2 0.2085942 0.0435116\n",
      "Bearing2_5 Bearing1 Bearing2 0.2182666 0.0476403\n",
      "Bearing2_5 Bearing2 None 0.2225948 0.0495484\n",
      "Bearing2_5 Bearing3 Bearing2 0.2044718 0.0418087\n",
      "Bearing2_6 Bearing1 Bearing2 0.1893502 0.0358535\n",
      "Bearing2_6 Bearing2 None 0.1859750 0.0345867\n",
      "Bearing2_6 Bearing3 Bearing2 0.1744119 0.0304195\n",
      "Bearing2_7 Bearing1 Bearing2 0.1935080 0.0374453\n",
      "Bearing2_7 Bearing2 None 0.2055573 0.0422538\n",
      "Bearing2_7 Bearing3 Bearing2 0.1856603 0.0344698\n",
      "Bearing3_3 Bearing1 Bearing3 0.0639432 0.0040887\n",
      "Bearing3_3 Bearing2 Bearing3 0.0955932 0.0091381\n",
      "Bearing3_3 Bearing3 None 0.0547699 0.0029997\n"
     ]
    }
   ],
   "source": [
    "test_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
