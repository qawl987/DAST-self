{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from scipy import interpolate\n",
    "import scipy.io as sio\n",
    "from numpy import *\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "max_window_stamp = 50\n",
    "window_size = 40\n",
    "train_acc = []\n",
    "train_temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_acc_temp(folder_type, bearing_folder, acc_offset, temp_offset):\n",
    "    csv_files = os.listdir(f'./{folder_type}/{bearing_folder}')\n",
    "    csv_files.sort()\n",
    "    acc_files = [acc for acc in csv_files if acc.startswith('acc')]\n",
    "    for i in range(1,acc_offset):\n",
    "        remove_name = 'acc_' + str(i).zfill(5) + '.csv'\n",
    "        acc_files.remove(remove_name)\n",
    "    temp_files = [temp for temp in csv_files if temp.startswith('temp')]\n",
    "    # print(len(acc_files))\n",
    "    # print(len(temp_files))\n",
    "    return acc_files, temp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(folder_type, bearing_folder, acc_offset, acc_files):\n",
    "    for stamp, acc_file in enumerate(acc_files):\n",
    "        vibra = pd.read_csv(f'./{folder_type}/{bearing_folder}/{acc_file}',header=None,sep=',', usecols=[4,5])\n",
    "        for i in range(max_window_stamp):\n",
    "            train_acc.append(vibra.iloc[i:i + window_size, :].values.tolist())\n",
    "            train_rul = len(acc_files) - acc_offset - stamp\n",
    "            train_y.append(train_rul)\n",
    "        # print(stamp)\n",
    "    return train_acc, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temp(folder_type, bearing_folder, temp_offset, temp_files):\n",
    "    for stamp, temp_file in enumerate(temp_files):\n",
    "        temp_csv = pd.read_csv(f'./{folder_type}/{bearing_folder}/{temp_file}',header=None,sep=',', usecols=[4])\n",
    "        for time in range(temp_offset, len(temp_csv), 100):\n",
    "            for i in range(max_window_stamp):\n",
    "                # 這邊用的time window 沒有對上\n",
    "                temp = int(temp_csv.iloc[time].values)\n",
    "                temp_list = [temp] * 40\n",
    "                train_temp.append(temp_list)\n",
    "    for i in range(max_window_stamp):\n",
    "        temp = int(temp_csv.iloc[time].values)\n",
    "        temp_list = [temp] * 40\n",
    "        train_temp.append(temp_list)\n",
    "    return train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mat(folder_type, bearing_folder, train_x, train_y):\n",
    "    sio.savemat(f'./Mat_train/{bearing_folder}_nor_train_x.mat', {\"train_x\": train_x})\n",
    "    sio.savemat(f'./Mat_train/{bearing_folder}_nor_train_y.mat', {\"train_y\": train_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(folder_type, bearing_folder, acc_offset, temp_offset):\n",
    "    acc_files, temp_files = read_acc_temp(folder_type, bearing_folder, acc_offset, temp_offset)\n",
    "    train_acc, train_y = get_acc(folder_type, bearing_folder, acc_offset, acc_files)\n",
    "    print(shape(train_acc))\n",
    "    train_temp = get_temp(folder_type, bearing_folder, temp_offset, temp_files)\n",
    "    print(shape(train_temp))\n",
    "    train_x = np.concatenate([train_acc, np.expand_dims(train_temp, axis=-1)], axis=-1)\n",
    "    first_dim = len(train_x)\n",
    "    train_x = train_x.reshape(-1, 3)\n",
    "    train_x = min_max_scaler.fit_transform(train_x)\n",
    "    train_x = np.reshape(train_x,(first_dim, 40, 3))\n",
    "    save_mat(folder_type, bearing_folder, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(train_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_offset = {'Bearing1_1': 8, 'Bearing1_2':7}\n",
    "temp_offset = {'Bearing1_1': 16, 'Bearing1_2': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_type = 'Learning_set'\n",
    "bearing_folder = 'Bearing1_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43250, 40, 2)\n",
      "(43250, 40)\n"
     ]
    }
   ],
   "source": [
    "train_preprocess(folder_type, bearing_folder, acc_offset[bearing_folder], temp_offset[bearing_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "print(shape(train_x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_offset = 12\n",
    "temp_offset = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_type = 'Test_set'\n",
    "bearing_folder = 'Bearing1_4'\n",
    "csv_files = os.listdir(f'./{folder_type}/{bearing_folder}')\n",
    "csv_files.sort()\n",
    "acc_files = [acc for acc in csv_files if acc.startswith('acc')]\n",
    "for i in range(1,acc_offset):\n",
    "    remove_name = 'acc_' + str(i).zfill(5) + '.csv'\n",
    "    acc_files.remove(remove_name)\n",
    "temp_files = [temp for temp in csv_files if temp.startswith('temp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "test_y = []\n",
    "max_window_stamp = 50\n",
    "window_size = 40\n",
    "test_acc = []\n",
    "test_temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_file = acc_files[-1]\n",
    "vibra = pd.read_csv(f'./{folder_type}/{bearing_folder}/{acc_file}',header=None,sep=',', usecols=[4,5])\n",
    "for i in range(max_window_stamp):\n",
    "    test_acc.append(vibra.iloc[i*25:i*25 + window_size, :].values.tolist())\n",
    "    test_y.append(34)\n",
    "# test_acc.append(vibra.iloc[-window_size: , :].values.tolist())\n",
    "# test_rul = len(acc_files) - acc_offset - stamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = temp_files[-1]\n",
    "temp_csv = pd.read_csv(f'./{folder_type}/{bearing_folder}/{temp_file}',header=None,sep=',', usecols=[4])\n",
    "for i in range(max_window_stamp):\n",
    "    temp = int(temp_csv.iloc[500 + temp_offset].values)\n",
    "    temp_list = [temp] * 40\n",
    "    test_temp.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(test_acc))\n",
    "print(shape(test_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.concatenate([test_acc, np.expand_dims(test_temp, axis=-1)], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_x.reshape(-1, 3)\n",
    "test_x = min_max_scaler.transform(test_x)\n",
    "test_x = np.reshape(test_x,(50, 40, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat(f'./Mat_test/{bearing_folder}_test_50stamp_x.mat', {\"test_x\": test_x})\n",
    "sio.savemat(f'./Mat_test/{bearing_folder}_test_50stamp_y.mat', {\"test_y\": test_y})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(f'./{folder_type}/{bearing_folder}/acc_00001.csv',header=None,sep=',', usecols=[4,5])\n",
    "a = pd.DataFrame([[1,2,3], [4,5,6]])\n",
    "b = a.iloc[0:2, 0:1]\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_temp))\n",
    "print(shape(train_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shape(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_acc))\n",
    "print(shape(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('./Learning_set/Bearing1_1/acc_00001.csv',\n",
    "            header=None,sep=',')\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(d.iloc[:,-2])\n",
    "plt.title('Horizontal_vibration_signals')\n",
    "plt.subplot(122)\n",
    "plt.plot(d.iloc[:,-1])\n",
    "plt.title('Vertical_vibration_signals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_bearings_data(folder):\n",
    "    ''' 获取某个工况下某个轴承的全部n个csv文件中的数据，返回numpy数组\n",
    "    dp:bearings_x_x的folder\n",
    "    return:folder下n个csv文件中的数据，shape:[n*32768,2]=[文件个数*采样点数，通道数]\n",
    "    '''\n",
    "    names = os.listdir(folder)\n",
    "    is_acc = ['acc' in name for name in names] \n",
    "    names = names[:sum(is_acc)]\n",
    "    files = [os.path.join(folder,f) for f in names]\n",
    "    # Bearing1_4 的csv文件的分隔符是分号：';'\n",
    "    print(pd.read_csv(files[0],header=None).shape)\n",
    "    sep = ';' if pd.read_csv(files[0],header=None).shape[-1]==1 else ','\n",
    "    h = [pd.read_csv(f,header=None,sep=sep).iloc[:,-2] for f in files]\n",
    "    v = [pd.read_csv(f,header=None,sep=sep).iloc[:,-1] for f in files]\n",
    "    H = np.concatenate(h)\n",
    "    V = np.concatenate(v)\n",
    "    print(H.shape,V.shape)\n",
    "    return np.stack([H,V],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'D:\\\\ysu\\\\databank\\\\phm_data'\n",
    "\n",
    "for i in ['Learning_set','Full_Test_Set']:\n",
    "    pp = os.path.join(p,i)\n",
    "    for j in os.listdir(pp):\n",
    "        ppp = os.path.join(pp,j)\n",
    "        print(ppp)\n",
    "        data = get_a_bearings_data(ppp)\n",
    "        save_name = p + '\\\\mat\\\\' + j+'.mat'\n",
    "        print(save_name)\n",
    "        scipy.io.savemat(save_name,{'h':data[:,0], 'v':data[:,1]}) # 写入mat文件        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset\n",
    "RUL_F001 = np.loadtxt('./Learn/Bearing1_1/')\n",
    "train_F001 = np.loadtxt('./cmapss/train_FD001.txt')\n",
    "test_F001 = np.loadtxt('./cmapss/test_FD001.txt')\n",
    "train_F001[:, 2:] = min_max_scaler.fit_transform(train_F001[:, 2:])\n",
    "test_F001[:, 2:] = min_max_scaler.transform(test_F001[:, 2:])\n",
    "train_01_nor = train_F001\n",
    "test_01_nor = test_F001\n",
    "print(train_01_nor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete worthless sensors\n",
    "train_01_nor = np.delete(train_01_nor, [5, 9, 10, 14, 20, 22, 23], axis=1) \n",
    "test_01_nor = np.delete(test_01_nor, [5, 9, 10, 14, 20, 22, 23], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters of data process\n",
    "RUL_max = 125.0  \n",
    "window_Size = 40 \n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "trainY_bu = []\n",
    "testX = []\n",
    "testY = []\n",
    "testY_bu = []\n",
    "testInd = []\n",
    "testLen = []\n",
    "testX_all = []\n",
    "testY_all = []\n",
    "test_len = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出1~編號上限的引擎\n",
    "for i in range(1, int(np.max(train_01_nor[:, 0])) + 1):\n",
    "    # 用np.where取出一個tuple, array存的是所有編號為i的引擎數據\n",
    "    ind = np.where(train_01_nor[:, 0] == i)\n",
    "    # 取出tuple 中的ndarray\n",
    "    ind = ind[0]\n",
    "    data_temp = train_01_nor[ind, :]\n",
    "    for j in range(len(data_temp) - window_Size + 1): \n",
    "        trainX.append(data_temp[j:j + window_Size, 2:].tolist())\n",
    "        # j+windowsize 代表目前time total-current=RUL\n",
    "        train_RUL = len(data_temp) - window_Size - j\n",
    "        # 看目前的rul跟max差多少\n",
    "        train_bu = RUL_max - train_RUL\n",
    "        if train_RUL > RUL_max:\n",
    "            train_RUL = RUL_max\n",
    "            train_bu = 0.0\n",
    "        trainY.append(train_RUL)\n",
    "        trainY_bu.append(train_bu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set sliding time window processing\n",
    "for i in range(1, int(np.max(test_01_nor[:, 0])) + 1):\n",
    "    ind = np.where(test_01_nor[:, 0] == i)\n",
    "    ind = ind[0]\n",
    "    # 編號i的engine的total time\n",
    "    testLen.append(float(len(ind)))\n",
    "    data_temp = test_01_nor[ind, :]\n",
    "    # 編號i的最後time step\n",
    "    testY_bu.append(data_temp[-1, 1])\n",
    "    if len(data_temp) < window_Size:\n",
    "        \n",
    "        data_temp_a = []\n",
    "        # 特徵維度\n",
    "        for myi in range(data_temp.shape[1]):\n",
    "            # 根據 x數量 產生 0 ~ 40之間的x座標 \n",
    "            x1 = np.linspace(0, window_Size - 1, len(data_temp))\n",
    "            # 產生 0~40x座標\n",
    "            x_new = np.linspace(0, window_Size - 1, window_Size)\n",
    "            # 根據維度myi的點產生逼近函數\n",
    "            tck = interpolate.splrep(x1, data_temp[:, myi])\n",
    "            # 將0~40帶入tck產生的函數得y點\n",
    "            a = interpolate.splev(x_new, tck)\n",
    "            data_temp_a.append(a.tolist())\n",
    "        data_temp_a = np.array(data_temp_a)\n",
    "        data_temp = data_temp_a.T\n",
    "        data_temp = data_temp[:, 2:]\n",
    "    else:\n",
    "        # test 只要最後四十個預測還有多少RUL即可\n",
    "        data_temp = data_temp[-window_Size:, 2:]  \n",
    "    # for concate usage extand 0th dimension\n",
    "    data_temp = np.reshape(data_temp, (1, data_temp.shape[0], data_temp.shape[1])) \n",
    "    testX = data_temp if i == 1 else np.concatenate((testX, data_temp), axis=0)\n",
    "    if RUL_F001[i - 1] > RUL_max:\n",
    "        testY.append(RUL_max)\n",
    "        #testY_bu.append(0.0)\n",
    "    else:\n",
    "        testY.append(RUL_F001[i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.array(trainX)\n",
    "testX = np.array(testX)\n",
    "trainY = np.array(trainY)/RUL_max \n",
    "trainY_bu = np.array(trainY_bu)/RUL_max\n",
    "testY = np.array(testY)/RUL_max\n",
    "testY_bu = np.array(testY_bu)/RUL_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainY.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('F001_window_size_trainX.mat', {\"train1X\": trainX})\n",
    "sio.savemat('F001_window_size_trainY.mat', {\"train1Y\": trainY})\n",
    "sio.savemat('F001_window_size_testX.mat', {\"test1X\": testX})\n",
    "sio.savemat('F001_window_size_testY.mat', {\"test1Y\": testY})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "mat = scipy.io.loadmat('F001_window_size_trainX.mat')\n",
    "struct_array = mat['']\n",
    "value = struct_array[0][0]['field_name']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30bcecd2be579928454f23a335de72e6ac700ea152f8294e8f6fe0cbdd0121a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
